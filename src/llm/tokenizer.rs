use anyhow::{Error, Result};

/// This is a wrapper around a tokenizer to ensure that tokens can be returned to the user in a
/// streaming way rather than having to wait for the full decoding.
pub struct TokenOutputStream {
    tokenizer: tokenizers::Tokenizer,
    tokens: Vec<u32>,
    prev_index: usize,
    current_index: usize,
}


impl TokenOutputStream {
     /// Constructs a new `TokenOutputStream`.
    ///
    /// # Arguments
    /// * `tokenizer` - A `tokenizers::Tokenizer` instance to use for decoding.
    ///
    /// # Returns
    /// An instance of `TokenOutputStream`.
    pub fn new(tokenizer: tokenizers::Tokenizer) -> Self {
        Self {
            tokenizer,
            tokens: Vec::new(),
            prev_index: 0,
            current_index: 0,
        }
    }

    /// Decodes a slice of token IDs into a string.
    ///
    /// # Arguments
    /// * `tokens` - A slice of token IDs to decode.
    ///
    /// # Returns
    /// A `Result` which, on success, contains the decoded string, and on failure, propagates the error with a descriptive error message.
    fn decode(&self, tokens: &[u32]) -> Result<String> {
        self.tokenizer.decode(tokens, true).map_err(Error::msg)
    }

    /// Processes the next token and returns the new text element generated by this token, if any.
    /// https://github.com/huggingface/text-generation-inference/blob/5ba53d44a18983a4de32d122f4cb46f4a17d9ef6/server/text_generation_server/models/model.py#L68
    ///
    /// # Arguments
    /// * `token` - The next token ID to process.
    ///
    /// # Returns
    /// A `Result` which, on success, contains an `Option<String>` representing the newly decoded text or `None` if no new text was generated.
    pub fn next_token(&mut self, token: u32) -> Result<Option<String>> {
        let prev_text = if self.tokens.is_empty() {
            String::new()
        } else {
            let tokens = &self.tokens[self.prev_index..self.current_index];
            self.decode(tokens)?
        };
        self.tokens.push(token);
        let text = self.decode(&self.tokens[self.prev_index..])?;
        if text.len() > prev_text.len() && text.chars().last().unwrap().is_alphanumeric() {
            let text = text.split_at(prev_text.len());
            self.prev_index = self.current_index;
            self.current_index = self.tokens.len();
            Ok(Some(text.1.to_string()))
        } else {
            Ok(None)
        }
    }

    /// Decodes the remaining tokens from the last checkpoint to the current position and returns the generated text, if any.
    ///
    /// # Returns
    /// A `Result` which, on success, contains an `Option<String>` with the new text, or `None` if no additional text was generated.
    pub fn decode_rest(&self) -> Result<Option<String>> {
        let prev_text = if self.tokens.is_empty() {
            String::new()
        } else {
            let tokens = &self.tokens[self.prev_index..self.current_index];
            self.decode(tokens)?
        };
        let text = self.decode(&self.tokens[self.prev_index..])?;
        if text.len() > prev_text.len() {
            let text = text.split_at(prev_text.len());
            Ok(Some(text.1.to_string()))
        } else {
            Ok(None)
        }
    }

    /// Provides a reference to the internal `Tokenizer` used by the stream.
    ///
    /// # Returns
    /// A reference to the `tokenizers::Tokenizer`.
    pub fn tokenizer(&self) -> &tokenizers::Tokenizer {
        &self.tokenizer
    }
}