use tokenizers::Tokenizer;
use anyhow::{Error, Result};


/// Loads a `Tokenizer` from a specified file path.
/// 
/// # Arguments
/// * `tokenizer_path` - A string slice that holds the file path to the tokenizer model.
///
/// # Returns
/// A `Result` which, on success, contains the `Tokenizer`, and on failure, contains an `Error`.
pub fn load_tokenizer(tokenizer_path: &str) -> Result<Tokenizer> {
    let tokenizer_path = std::path::PathBuf::from(tokenizer_path);
    Tokenizer::from_file(tokenizer_path).map_err(Error::msg)
}

/// This is a wrapper around a tokenizer to ensure that tokens can be returned to the user in a
/// streaming way rather than having to wait for the full decoding.
pub struct TokenOutputStream {
    tokenizer: tokenizers::Tokenizer,
    tokens: Vec<u32>,
    prev_index: usize,
    current_index: usize,
}


impl TokenOutputStream {
     /// Constructs a new `TokenOutputStream`.
    ///
    /// # Arguments
    /// * `tokenizer` - A `tokenizers::Tokenizer` instance to use for decoding.
    ///
    /// # Returns
    /// An instance of `TokenOutputStream`.
    pub fn new(tokenizer: tokenizers::Tokenizer) -> Self {
        Self {
            tokenizer,
            tokens: Vec::new(),
            prev_index: 0,
            current_index: 0,
        }
    }

    /// Decodes a slice of token IDs into a string.
    ///
    /// # Arguments
    /// * `tokens` - A slice of token IDs to decode.
    ///
    /// # Returns
    /// A `Result` which, on success, contains the decoded string, and on failure, propagates the error with a descriptive error message.
    fn decode(&self, tokens: &[u32]) -> Result<String> {
        self.tokenizer.decode(tokens, true).map_err(Error::msg)
    }

    /// Processes the next token and returns the new text element generated by this token, if any.
    /// https://github.com/huggingface/text-generation-inference/blob/5ba53d44a18983a4de32d122f4cb46f4a17d9ef6/server/text_generation_server/models/model.py#L68
    ///
    /// # Arguments
    /// * `token` - The next token ID to process.
    ///
    /// # Returns
    /// A `Result` which, on success, contains an `Option<String>` representing the newly decoded text or `None` if no new text was generated.
    pub fn next_token(&mut self, token: u32) -> Result<Option<String>> {
        let prev_text = if self.tokens.is_empty() {
            String::new()
        } else {
            let tokens = &self.tokens[self.prev_index..self.current_index];
            self.decode(tokens)?
        };
        self.tokens.push(token);
        let text = self.decode(&self.tokens[self.prev_index..])?;
        if text.len() > prev_text.len() && text.chars().last().unwrap().is_alphanumeric() {
            let text = text.split_at(prev_text.len());
            self.prev_index = self.current_index;
            self.current_index = self.tokens.len();
            Ok(Some(text.1.to_string()))
        } else {
            Ok(None)
        }
    }

    /// Decodes the remaining tokens from the last checkpoint to the current position and returns the generated text, if any.
    ///
    /// # Returns
    /// A `Result` which, on success, contains an `Option<String>` with the new text, or `None` if no additional text was generated.
    pub fn decode_rest(&self) -> Result<Option<String>> {
        let prev_text = if self.tokens.is_empty() {
            String::new()
        } else {
            let tokens = &self.tokens[self.prev_index..self.current_index];
            self.decode(tokens)?
        };
        let text = self.decode(&self.tokens[self.prev_index..])?;
        if text.len() > prev_text.len() {
            let text = text.split_at(prev_text.len());
            Ok(Some(text.1.to_string()))
        } else {
            Ok(None)
        }
    }

    /// Decodes all tokens stored in the stream to a single string.
    ///
    /// # Returns
    /// A `Result` containing the fully decoded string.
    pub fn decode_all(&self) -> Result<String> {
        self.decode(&self.tokens)
    }

    /// Retrieves the token ID for a given string, if it exists in the tokenizer's vocabulary.
    ///
    /// # Arguments
    /// * `token_s` - The string representation of the token to look up.
    ///
    /// # Returns
    /// An `Option<u32>` which contains the token ID if found, or `None` if not found.
    pub fn get_token(&self, token_s: &str) -> Option<u32> {
        self.tokenizer.get_vocab(true).get(token_s).copied()
    }

    /// Provides a reference to the internal `Tokenizer` used by the stream.
    ///
    /// # Returns
    /// A reference to the `tokenizers::Tokenizer`.
    pub fn tokenizer(&self) -> &tokenizers::Tokenizer {
        &self.tokenizer
    }

    /// Provides a reference to the internal `Tokenizer` used by the stream.
    ///
    /// # Returns
    /// A reference to the `tokenizers::Tokenizer`.
    pub fn clear(&mut self) {
        self.tokens.clear();
        self.prev_index = 0;
        self.current_index = 0;
    }
}